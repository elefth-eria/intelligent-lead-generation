{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f029fa8d-7b3b-428a-97ae-b6d137daeb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import requests, re, os\n",
    "from bs4 import BeautifulSoup\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import pycountry\n",
    "\n",
    "# UI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import pipeline\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edbc6149-df1c-4a53-9836-2986a90dc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "from typing import Optional, List\n",
    "\n",
    "def _country_to_cc(country: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Accepts 'Greece', 'GR', 'gr', etc. Returns 'gr' for GL/ccTLD usage.\n",
    "    Falls back to 'com' when unknown.\n",
    "    \"\"\"\n",
    "    if not country:\n",
    "        return \"com\"\n",
    "    try:\n",
    "        rec = pycountry.countries.lookup(str(country).strip())\n",
    "        return rec.alpha_2.lower()\n",
    "    except Exception:\n",
    "        s = str(country).strip()\n",
    "        return s.lower() if len(s) == 2 else \"com\"\n",
    "\n",
    "def _country_name(country: Optional[str]) -> str:\n",
    "    if not country:\n",
    "        return \"\"\n",
    "    try:\n",
    "        rec = pycountry.countries.lookup(str(country).strip())\n",
    "        # Use common_name if present, else name\n",
    "        return getattr(rec, \"common_name\", rec.name)\n",
    "    except Exception:\n",
    "        return str(country)\n",
    "\n",
    "# Optional demonym/alias map to strengthen country text matches\n",
    "_DEMONYM_MAP = {\n",
    "    \"gr\": [\"greece\", \"greek\", \"hellas\", \"hellenic\"],\n",
    "    \"de\": [\"germany\", \"german\", \"deutschland\"],\n",
    "    \"es\": [\"spain\", \"spanish\", \"españa\"],\n",
    "    \"fr\": [\"france\", \"french\", \"français\"],\n",
    "    \"it\": [\"italy\", \"italian\", \"italia\"],\n",
    "    \"pt\": [\"portugal\", \"portuguese\", \"português\"],\n",
    "    \"nl\": [\"netherlands\", \"dutch\", \"holland\"],\n",
    "    \"se\": [\"sweden\", \"swedish\", \"sverige\"],\n",
    "    \"fi\": [\"finland\", \"finnish\", \"suomi\"],\n",
    "    \"no\": [\"norway\", \"norwegian\", \"norge\"],\n",
    "    \"dk\": [\"denmark\", \"danish\", \"danmark\"],\n",
    "    \"pl\": [\"poland\", \"polish\", \"polska\"],\n",
    "    \"cz\": [\"czech republic\", \"czech\", \"česko\"],\n",
    "    \"hu\": [\"hungary\", \"hungarian\", \"magyar\"],\n",
    "    \"tr\": [\"turkey\", \"turkish\", \"türkiye\"],\n",
    "    \"uk\": [\"ukraine\", \"ukrainian\"],\n",
    "    \"gb\": [\"united kingdom\", \"uk\", \"britain\", \"british\", \"england\", \"scotland\", \"wales\"],\n",
    "    \"ie\": [\"ireland\", \"irish\", \"éire\"],\n",
    "    \"ro\": [\"romania\", \"romanian\", \"românia\"]\n",
    "}\n",
    "\n",
    "def _country_clause(country: Optional[str]) -> str:\n",
    "    \"\"\"\n",
    "    Returns a clause like: (greece OR greek OR hellas OR hellenic)\n",
    "    Uses demonyms if we know them; otherwise just the country name.\n",
    "    \"\"\"\n",
    "    cc = _country_to_cc(country)\n",
    "    if cc == \"com\":\n",
    "        return \"\"  # global search\n",
    "    terms = _DEMONYM_MAP.get(cc)\n",
    "    if not terms:\n",
    "        name = _country_name(country)\n",
    "        terms = [name.lower()]\n",
    "    joined = \" OR \".join(sorted(set(t.lower() for t in terms)))\n",
    "    return f\"({joined})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7375094a-43d0-48c9-b254-d57899bb41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def _base_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the base URL (scheme://netloc).\n",
    "    Example: https://www.abcfund.gr/about -> https://www.abcfund.gr\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "    except Exception:\n",
    "        return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9546dea5-be5d-47a2-85f3-86efdf69edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Synonym Agent ----\n",
    "class SmartSynonymAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        device: int = -1,          # -1 = CPU, 0 = GPU\n",
    "        max_new_tokens: int = 64,\n",
    "        temperature: float = 0.3,  # low = more deterministic synonyms\n",
    "        top_p: float = 0.9\n",
    "    ):\n",
    "        self.generator = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model_name,\n",
    "            device=device\n",
    "        )\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "\n",
    "    def expand(self, topic: str, user_company: str = \"\", linkedin_text: str = \"\") -> List[str]:\n",
    "        prompt = (\n",
    "            f\"List exactly 12 different synonyms or alternative search phrases for the topic: {topic}. \"\n",
    "            f\"Do not repeat the topic itself. \"\n",
    "            \"Respond only with a plain comma-separated list of terms. \"\n",
    "            \"No numbering, no quotes, no explanations, no filler text.\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            out = self.generator(\n",
    "                prompt,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=self.temperature,\n",
    "                top_p=self.top_p\n",
    "            )[0][\"generated_text\"]\n",
    "\n",
    "            raw = out.split(prompt, 1)[-1].strip()\n",
    "\n",
    "            terms = [\n",
    "                t.strip().lower().strip('\"').strip(\"'\")\n",
    "                for t in raw.split(\",\")\n",
    "                if 2 < len(t.strip()) < 40\n",
    "            ]\n",
    "            if not terms:\n",
    "                terms = [topic.lower()]\n",
    "\n",
    "            # Deduplicate, preserve order, always include the topic\n",
    "            seen, uniq = set(), []\n",
    "            for t in [topic.lower()] + terms:\n",
    "                if t and t not in seen:\n",
    "                    seen.add(t)\n",
    "                    uniq.append(t)\n",
    "\n",
    "            print(f\"[DEBUG] SmartSynonymAgent.expand('{topic}') -> {uniq}\")\n",
    "            return uniq\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] SmartSynonymAgent failed: {e}\")\n",
    "            return [topic.lower()]\n",
    "\n",
    "# ---- Keyword Agent ----\n",
    "\n",
    "@dataclass\n",
    "class KeywordAgent:\n",
    "    synonym_agent: SmartSynonymAgent\n",
    "\n",
    "    max_queries: int = 14\n",
    "\n",
    "    INTENTS = ['\"official site\"','\"about us\"']\n",
    "\n",
    "    def generate(self, topic: str, country_code: Optional[str] = None) -> List[str]:\n",
    "        synonyms = self.synonym_agent.expand(topic)\n",
    "        cc = _country_to_cc(country_code) if country_code else \"com\"\n",
    "        country_txt = _country_clause(country_code)  # e.g., (greece OR greek OR hellas OR hellenic)\n",
    "        site_clause = f\"(site:.{cc} OR site:.com)\" if cc != \"com\" else \"site:.com\"\n",
    "\n",
    "        queries: List[str] = []\n",
    "        for term in synonyms:\n",
    "            intent_clause = \"(\" + \" OR \".join(self.INTENTS) + \")\"\n",
    "            # Quote the term to keep phrase matching\n",
    "            if country_txt:\n",
    "                q = f'\"{term}\" {intent_clause} {country_txt} {site_clause}'\n",
    "            else:\n",
    "                q = f'\"{term}\" {intent_clause} {site_clause}'\n",
    "            queries.append(q)\n",
    "\n",
    "        seen, uniq = set(), []\n",
    "        for q in queries:\n",
    "            if q not in seen:\n",
    "                seen.add(q)\n",
    "                uniq.append(q)\n",
    "            if len(uniq) >= self.max_queries:\n",
    "                break\n",
    "\n",
    "        print(f\"[DEBUG] KeywordAgent.generate('{topic}', country='{country_code}') -> {len(uniq)} queries\")\n",
    "        return uniq\n",
    "\n",
    "\n",
    "# ---- SerpApi Search Agent ----\n",
    "class SerpApiSearchAgent:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://serpapi.com/search\"\n",
    "\n",
    "    def search(self, queries: List[str], country: str) -> List[dict]:\n",
    "        results: List[dict] = []\n",
    "        seen_bases = set()\n",
    "        gl = _country_to_cc(country)\n",
    "\n",
    "        for q in queries:\n",
    "            params = {\"engine\": \"google\", \"q\": q, \"hl\": \"en\", \"gl\": gl, \"api_key\": self.api_key}\n",
    "            try:\n",
    "                data = requests.get(self.base_url, params=params, timeout=20).json()\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] SerpApi request failed for '{q}': {e}\")\n",
    "                continue\n",
    "\n",
    "            for r in (data.get(\"organic_results\") or []):\n",
    "                link = r.get(\"link\") or r.get(\"url\")\n",
    "                if not link:\n",
    "                    continue\n",
    "\n",
    "                base = _base_url(link)\n",
    "                if base in seen_bases:\n",
    "                    continue  # skip duplicates by base domain\n",
    "\n",
    "                seen_bases.add(base)\n",
    "                results.append({\n",
    "                    \"title\": r.get(\"title\") or \"\",\n",
    "                    \"url\": link,\n",
    "                    \"base\": base\n",
    "                })\n",
    "\n",
    "        print(f\"[DEBUG] SerpApiSearchAgent.search -> {len(results)} unique base URLs\")\n",
    "        return results\n",
    "\n",
    "# ---- Filter Agent ----\n",
    "class FilterAgent:\n",
    "    def filter(self, results: List[dict]) -> List[dict]:\n",
    "        filtered: List[dict] = []\n",
    "        for r in results:\n",
    "            url = r.get(\"url\")\n",
    "            if not url:\n",
    "                continue\n",
    "            if any(block in url for block in [\"linkedin.com\", \"crunchbase.com\", \"wikipedia.org\"]):\n",
    "                continue\n",
    "            filtered.append(r)\n",
    "        print(f\"[DEBUG] FilterAgent.filter -> {len(filtered)} after filtering\")\n",
    "        return filtered\n",
    "\n",
    "\n",
    "# ---- Contact Page Finder ----\n",
    "\n",
    "class ContactPageFinder:\n",
    "    def __init__(self):\n",
    "        self.headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        self.email_pattern = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "        self.link_keywords = [\n",
    "            \"contact\", \"about\", \"support\", \"help\", \"customer-service\", \"press\", \"media\", \"get-in-touch\"\n",
    "        ]\n",
    "        self.fallback_paths = [\"/contact\", \"/about\", \"/press\", \"/support\", \"/help\"]\n",
    "\n",
    "    def fetch(self, url: str) -> Optional[str]:\n",
    "        try:\n",
    "            resp = requests.get(url, headers=self.headers, timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                return resp.text\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to fetch {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def extract_emails(self, html: str) -> List[str]:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        text_blocks = soup.find_all(string=True)\n",
    "        raw_text = \" \".join(t.strip() for t in text_blocks if t.strip())\n",
    "        return list(set(self.email_pattern.findall(raw_text)))\n",
    "\n",
    "    def find_relevant_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "        links = []\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"].lower()\n",
    "            if any(kw in href for kw in self.link_keywords):\n",
    "                if href.startswith(\"http\"):\n",
    "                    links.append(href)\n",
    "                elif href.startswith(\"/\"):\n",
    "                    links.append(base_url.rstrip(\"/\") + href)\n",
    "        return links\n",
    "\n",
    "    def find(self, base_url: str) -> dict:\n",
    "        homepage_html = self.fetch(base_url)\n",
    "        if not homepage_html:\n",
    "            return {\"name\": None, \"homepage\": base_url, \"email\": None}\n",
    "\n",
    "        soup = BeautifulSoup(homepage_html, \"html.parser\")\n",
    "        name = soup.title.string.strip() if soup.title and soup.title.string else base_url\n",
    "\n",
    "        #Try homepage\n",
    "        emails = self.extract_emails(homepage_html)\n",
    "\n",
    "        #Discover links from homepage\n",
    "        candidate_links = self.find_relevant_links(soup, base_url)\n",
    "\n",
    "        #fallback\n",
    "        for path in self.fallback_paths:\n",
    "            fallback_url = base_url.rstrip(\"/\") + path\n",
    "            if fallback_url not in candidate_links:\n",
    "                candidate_links.append(fallback_url)\n",
    "\n",
    "        #scan all links until email is found\n",
    "        for link in candidate_links:\n",
    "            linked_html = self.fetch(link)\n",
    "            if linked_html:\n",
    "                found = self.extract_emails(linked_html)\n",
    "                if found:\n",
    "                    emails.extend(found)\n",
    "                    break\n",
    "\n",
    "        email = emails[0] if emails else None\n",
    "        return {\"name\": name, \"homepage\": base_url, \"email\": email} \n",
    "        \n",
    "class PersonalizedSearchAgent:\n",
    "    def __init__(self, user_profile: dict, keyword_agent, search_agent, filter_agent):\n",
    "        self.user_profile = user_profile or {}\n",
    "        self.keyword_agent = keyword_agent\n",
    "        self.search_agent = search_agent\n",
    "        self.filter_agent = filter_agent\n",
    "\n",
    "    def search(self, topic: str, country_code: str, max_results: int = 10):\n",
    "        # Build base queries\n",
    "        base_queries = self.keyword_agent.generate(topic, country_code=country_code)\n",
    "\n",
    "        # Personalization: add negative terms to avoid your own name/company/LinkedIn handle\n",
    "        neg_terms = []\n",
    "        full_name = f\"{self.user_profile.get('name','').strip()} {self.user_profile.get('surname','').strip()}\".strip()\n",
    "        if full_name:\n",
    "            neg_terms.append(full_name)\n",
    "        if self.user_profile.get('company'):\n",
    "            neg_terms.append(self.user_profile['company'])\n",
    "        lnk = self.user_profile.get('linkedin','').strip()\n",
    "        if lnk:\n",
    "            handle = lnk.rsplit('/', 1)[-1]\n",
    "            if handle:\n",
    "                neg_terms.append(handle)\n",
    "\n",
    "        if neg_terms:\n",
    "            base_queries = [q + ''.join([f' -\\\"{t}\\\"' for t in neg_terms if t]) for q in base_queries]\n",
    "\n",
    "        results = self.search_agent.search(base_queries, country=country_code)\n",
    "        results = self.filter_agent.filter(results)\n",
    "\n",
    "        # Simple re-ranking: penalize self-signals; small boost for ccTLD\n",
    "        def _score(rec):\n",
    "            url = (rec.get('url') or '').lower()\n",
    "            title = (rec.get('title') or '').lower()\n",
    "            score = 0\n",
    "\n",
    "            company = (self.user_profile.get('company') or '').lower()\n",
    "            if company and company in (url + ' ' + title):\n",
    "                score -= 5\n",
    "\n",
    "            fn = (self.user_profile.get('name') or '').lower().strip()\n",
    "            ln = (self.user_profile.get('surname') or '').lower().strip()\n",
    "            if fn and ln and (fn in (url + ' ' + title)) and (ln in (url + ' ' + title)):\n",
    "                score -= 3\n",
    "\n",
    "            cc = _country_to_cc(country_code)\n",
    "            if cc != 'com' and (url.endswith('.' + cc) or ('.' + cc + '/' in url)):\n",
    "                score += 1\n",
    "\n",
    "            return score\n",
    "\n",
    "        results = sorted(results, key=_score, reverse=True)\n",
    "        return results[:max_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8a9b7a-aad1-48e9-b61c-fce73df06e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a8fbb288bb424b8cbb6802c87d0f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agents initialized and ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialization of Agents ---\n",
    "\n",
    "# 🔑 Replace this with your own SerpApi key\n",
    "API_KEY = \"your_key_here\" \n",
    "\n",
    "# Synonym Agent \n",
    "\n",
    "synonym_agent = SmartSynonymAgent(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    device=0\n",
    ")\n",
    "\n",
    "# Keyword Agent\n",
    "ka = KeywordAgent(synonym_agent=synonym_agent)\n",
    "\n",
    "\n",
    "# SerpApi Search Agent\n",
    "sa = SerpApiSearchAgent(api_key=API_KEY)\n",
    "\n",
    "# Filter Agent\n",
    "fa = FilterAgent()\n",
    "\n",
    "# Contact Page Finder (for optional email scraping)\n",
    "finder = ContactPageFinder()\n",
    "\n",
    "# Example user profile (UI will override this)\n",
    "user_profile = {\n",
    "    \"name\": \"George\",\n",
    "    \"surname\": \"Papanikolaou\",\n",
    "    \"company\": \"Panathenea\",\n",
    "    \"linkedin\": \"https://www.linkedin.com/in/geo\"\n",
    "}\n",
    "\n",
    "# Personalized Search Agent\n",
    "psa = PersonalizedSearchAgent(user_profile, ka, sa, fa)\n",
    "\n",
    "print(\"✅ Agents initialized and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9054b562-cc5f-4fb2-b89e-31adcf996b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a79cba81bfa4a279b5b62e35b47934a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='', description='First name:'), Text(value='', description='Surname:'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- UI Inputs ---\n",
    "first_name_input = widgets.Text(description=\"First name:\")\n",
    "surname_input = widgets.Text(description=\"Surname:\")\n",
    "company_input = widgets.Text(description=\"Company:\")\n",
    "linkedin_input = widgets.Text(description=\"LinkedIn:\")\n",
    "topic_input = widgets.Text(description=\"Topic:\")\n",
    "\n",
    "country_dropdown = widgets.Dropdown(\n",
    "    options=sorted([c.name for c in pycountry.countries]),\n",
    "    description=\"Country:\"\n",
    ")\n",
    "\n",
    "max_results_input = widgets.IntText(value=10, description=\"Max results:\")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Search\", button_style=\"success\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(\"⏳ Running search...\")\n",
    "\n",
    "        user_profile = {\n",
    "            \"name\": first_name_input.value.strip(),\n",
    "            \"surname\": surname_input.value.strip(),\n",
    "            \"company\": company_input.value.strip(),\n",
    "            \"linkedin\": linkedin_input.value.strip(),\n",
    "        }\n",
    "\n",
    "        psa = PersonalizedSearchAgent(user_profile, ka, sa, fa)\n",
    "\n",
    "        results = psa.search(\n",
    "            topic=topic_input.value.strip(),\n",
    "            country_code=country_dropdown.value,\n",
    "            max_results=int(max_results_input.value)\n",
    "        )\n",
    "        print(f\"🔍 {len(results)} results fetched before deduplication\")\n",
    "\n",
    "        # Deduplicate by URL\n",
    "        seen, unique_results = set(), []\n",
    "        for r in results:\n",
    "            if r[\"url\"] not in seen:\n",
    "                seen.add(r[\"url\"])\n",
    "                unique_results.append(r)\n",
    "        print(f\"📌 {len(unique_results)} unique results after deduplication\")\n",
    "\n",
    "        # Extract contact info\n",
    "        directory_rows = []\n",
    "        for r in tqdm(unique_results, desc=\"Fetching contact info\"):\n",
    "            info = finder.find(r[\"url\"])\n",
    "            directory_rows.append({\n",
    "                \"Name\": info.get(\"name\") or (r.get(\"title\") or \"Unknown\"),\n",
    "                \"Homepage\": f'<a href=\"{r[\"url\"]}\" target=\"_blank\">Homepage</a>',\n",
    "                \"Email\": info.get(\"email\") or \"No email found\"\n",
    "            })\n",
    "\n",
    "        # Display\n",
    "        df = pd.DataFrame(directory_rows)\n",
    "        display(HTML(df.to_html(escape=False, index=False)))\n",
    "\n",
    "        # Save\n",
    "        df.to_csv(\"directory_results.csv\", index=False)\n",
    "        print(\"✅ Results saved to directory_results.csv\")\n",
    "\n",
    "run_button.on_click(on_button_click)\n",
    "\n",
    "# Layout\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        widgets.HBox([first_name_input, surname_input]),\n",
    "        company_input,\n",
    "        linkedin_input,\n",
    "        topic_input,\n",
    "        widgets.HBox([country_dropdown, max_results_input]),\n",
    "        run_button,\n",
    "        output_area\n",
    "    ])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
